---
title: "4: Part 1 - Data Wrangling"
author: "Environmental Data Analytics | John Fay and Luana Lima | Developed by Kateri Salk"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Describe the usefulness of data wrangling and its place in the data pipeline
2. Wrangle datasets with dplyr functions
3. Apply data wrangling skills to a real-world example dataset


## Set up your session

Today we will work with a dataset from the [North Temperate Lakes Long-Term Ecological Research Station](https://lter.limnology.wisc.edu/about/overview). The NTL-LTER is located in the boreal zone in northern Wisconsin, USA. We will use the [chemical and physical limnology dataset](https://lter.limnology.wisc.edu/content/cascade-project-north-temperate-lakes-lter-core-data-physical-and-chemical-limnology-1984), running from 1984-2016. 

Opening discussion: why might we be interested in long-term observations of temperature, oxygen, and light in lakes?

> Add notes here: Chemical and physical limnology for lakes - we have onservations in temp, oxygen, and light. This is interesting because it tells us about the health and turn over of a lake system. 

```{r setup workspace, message = FALSE}
getwd()
#install.packages(tidyverse)
install.packages("tidyverse")
library(tidyverse)
#install.packages(lubridate)
library(lubridate)
#stringAsFactor = TRUE, summary will work for characters**
NTL.phys.data <- read.csv("./Data/Raw/NTL-LTER_Lake_ChemistryPhysics_Raw.csv", stringsAsFactors = TRUE)

#Check data was imported correctly
colnames(NTL.phys.data)
head(NTL.phys.data) #dataset that is recorded at different depths, so it will have different entries for each column (why there are so many dates that are same)
summary(NTL.phys.data)
str(NTL.phys.data) #what type of data is each column? #tells us the date is a factor not a date object
dim(NTL.phys.data)

class(NTL.phys.data$sampledate)
# Format sampledate as date
NTL.phys.data$sampledate <- as.Date(NTL.phys.data$sampledate, format = "%m/%d/%y")
#check is it a date?
class(NTL.phys.data$sampledate)
#using head function shows us all the dates in that column to make sure that dates aren't wonky
head(NTL.phys.data$sampledate)
```

## Data Wrangling

Data wrangling extends data exploration: it allows you to process data in ways that are useful for you. An important part of data wrangling is creating *tidy datasets*, with the following rules: 

Rule of thumb for tidy datasets:

1. Each variable has its own column
2. Each observation has its own row
3. Each value has its own cell

What is the best way to wrangle data? There are multiple ways to arrive at a specific outcome in R, and we will illustrate some of those approaches. Your goal should be to write the simplest code that will get you to your desired outcome. However, there is sometimes a trade-off of the opportunity cost to learn a new formulation of code and the time it takes to write complex code that you already know. Remember that the best code is one that is easy to understand for yourself and your collaborators. Remember to comment your code, use informative names for variables and functions, and use reproducible methods to arrive at your output.

## Dplyr Wrangling Functions

`dplyr` is a package in R that includes functions for data manipulation (i.e., data wrangling or data munging). `dplyr` is included in the tidyverse package, so you should already have it installed on your machine. The functions act as verbs for data wrangling processes. For more information, run this line of code:

--> This is one strategy for wrangling functions - use whatever is cleanest and makes sense to you!

```{r, results = "hide"}
#shows you what the package dplyr will do --> look Single table verbs section (particularly helpful)
vignette("dplyr")

#When you want to filter rows
#filter() - chooses rows based on column values
#slice() - chooses rows based on location
#arrange() - changes the order of the rows

#Columns
#select() - changes whether or not a column is included
#rename() - changes the names of columns
#mutate() - changes the values of columns and creates new columns
#relocate() - changes the order of the columns

```

### Filter

Filtering allows us to choose certain rows (observations) in our dataset.

Here are the relevant commands used in the `filter` function. Add some notes to designate what these commands mean. 
`==` -- compare value to a specific value
`!=` -- if variables are different
`<` -- less than a number
`<=` -- less than or equal to
`>` -- greater than
`>=` -- greater than or equal
`&` -- multiple conditions to check aggregate with &
`|` -- if you want both conditions to be true 

```{r filtering data}
class(NTL.phys.data$lakeid) 
#you can only compare character with character, numeric with numeric 
class(NTL.phys.data$depth)

# matrix filtering
#want to look at surface information
#filtering rows that have element 0 on column depth (== 0)
NTL.phys.data.surface1 <- NTL.phys.data[NTL.phys.data$depth == 0,]

# dplyr filtering
#how to filter just depths at 0 using dplyr
#first argument is dataset, 2nd argument is whatever you're filtering with the condition == 0
NTL.phys.data.surface2 <- filter(NTL.phys.data, depth == 0)
#you want depth above 0.25 or values that will be less than 0.25
NTL.phys.data.surface3 <- filter(NTL.phys.data, depth < 0.25)

# Did the methods arrive at the same result?
head(NTL.phys.data.surface1)
dim(NTL.phys.data.surface1)
head(NTL.phys.data.surface2)
dim(NTL.phys.data.surface2)
head(NTL.phys.data.surface3)
dim(NTL.phys.data.surface3)

# Choose multiple conditions to filter in one line
#number of occurrences of lake names - you can see the names that have the most
summary(NTL.phys.data$lakename)

#let's select just the top 2 lakes for further analysis
#using filter function with original data, using the two lakenames == to set the ones you want, and then using | as an "or" 
NTL.phys.data.PeterPaul1 <- filter(NTL.phys.data, lakename == "Paul Lake" | lakename == "Peter Lake")

#Method 2 --> could also check when the data isn't what you're looking at (i.e. excluding the other lakes we don't want to look at) 
#use ! to exclude the lakes
NTL.phys.data.PeterPaul2 <- filter(NTL.phys.data, lakename != "Central Long Lake" & 
                                     lakename != "Crampton Lake" & lakename != "East Long Lake" &
                                     lakename != "Hummingbird Lake" & lakename != "Tuesday Lake" &
                                     lakename != "Ward Lake" & lakename != "West Long Lake")

#Method 3
#useful when you need to select more than 2 elements 
#Instead of writing lakename == a lot, you can write lakename %in% and create a vector with all of the names you want
NTL.phys.data.PeterPaul3 <- filter(NTL.phys.data, lakename %in% c("Paul Lake", "Peter Lake"))

# Choose a range of conditions of a numeric or integer variable
#maybe you want days from June to October 
#Open data set and see at which number (day) do the data start where you want (i.e. 151 or 305)
summary(NTL.phys.data$daynum)
#152 is first day of June so > 151
#Use & because we want to say both 152 and it's less than 305
NTL.phys.data.JunethruOctober1 <- filter(NTL.phys.data, daynum > 151 & daynum < 305)

#comma is same thing as &
NTL.phys.data.JunethruOctober2 <- filter(NTL.phys.data, daynum > 151, daynum < 305)

#if you want to use >= you can start at 152 and <= 304
NTL.phys.data.JunethruOctober3 <- filter(NTL.phys.data, daynum >= 152 & daynum <= 304)

#also use the %in% to look at just the days in between the values we want by creating a vector of the values between 152:304
NTL.phys.data.JunethruOctober4 <- filter(NTL.phys.data, daynum %in% c(152:304))

# Exercise: 
# filter NTL.phys.data for the year 1999
# what code do you need to use, based on the class of the variable?
class(NTL.phys.data$year4)

# Exercise: 
# filter NTL.phys.data for Tuesday Lake from 1990 through 1999.


```
Question: Why don't we filter using row numbers?

> Answer: 

### Arrange

Arranging allows us to change the order of rows in our dataset. By default, the arrange function will arrange rows in ascending order.

```{r arranging data}
#need to specify which column you want to arrange by
NTL.phys.data.depth.ascending <- arrange(NTL.phys.data, depth)

#arrange in decending order - change second argument to desc (descending) with the column you want descending
NTL.phys.data.depth.descending <- arrange(NTL.phys.data, desc(depth))

# Exercise: 
# Arrange NTL.phys.data by temperature, in descending order. 
# Which dates, lakes, and depths have the highest temperatures?


```
### Select

Selecting allows us to choose certain columns (variables) in our dataset.

```{r selecting columns}
#allows us to choose certain columns in our dataset 
#create a new data frame 
#if we are just worried about the lakename and what the temperature is at those lakes - pulling out a set of data, lakename and everything that ranges in between sampledate and temperature_c
#can pull out a subset of data to make analysis easier
NTL.phys.data.temps <- select(NTL.phys.data, lakename, sampledate:temperature_C)

```
### Mutate

Mutating allows us to add new columns that are functions of existing columns. Operations include addition, subtraction, multiplication, division, log, and other functions.

```{r mutating data}
#say we want to include a new column that has temp but on new unit #say add temp but in Fahrenheit 
#whatever is on left of = is name of new column 
#if there isn't a temp available it will say N/A in column
NTL.phys.data.temps <- mutate(NTL.phys.data.temps, temperature_F = (temperature_C*9/5) + 32)

```

## Lubridate

A package that makes coercing date much easier is `lubridate`. A guide to the package can be found at https://lubridate.tidyverse.org/. The cheat sheet within that web page is excellent too. This package can do many things (hint: look into this package if you are having unique date-type issues), but today we will be using two of its functions for our NTL dataset. 

```{r using lubridate}
#going to extract just a month from a date object - useful if you want to aggregate data by month or summary by month 

# add a month column to the dataset
#month() adds a month
NTL.phys.data.PeterPaul1 <- mutate(NTL.phys.data.PeterPaul1, month = month(sampledate)) 

# reorder columns to put month with the rest of the date variables
#let's put month next to the rest of the date information
#select() can also change order of columns 
#figure out where you want month to go (right after daynum)
#select(dataset, columns before new column, data/columns after new column)
NTL.phys.data.PeterPaul1 <- select(NTL.phys.data.PeterPaul1, lakeid:daynum, month, sampledate:comments)

# find out the start and end dates of the dataset
#interval() - can check what the first and last objects

#CONFUSED ON THIS?? CAN WE GO OVER AGAIN?

interval(NTL.phys.data.PeterPaul1$sampledate[1], NTL.phys.data.PeterPaul1$sampledate[21613])
interval(first(NTL.phys.data.PeterPaul1$sampledate), last(NTL.phys.data.PeterPaul1$sampledate))
```


## Pipes

--> Helpful because you don't have to keep specifying the dataset

Sometimes we will want to perform multiple functions on a single dataset on our way to creating a processed dataset. We could do this in a series of subsequent functions or create a custom function. However, there is another method to do this that looks cleaner and is easier to read. This method is called a pipe. We designate a pipe with `%>%`. A good way to think about the function of a pipe is with the word "then." 

Let's say we want to take our raw dataset (NTL.phys.data), *then* filter the data for Peter and Paul lakes, *then* select temperature and observation information, and *then* add a column for temperature in Fahrenheit: 

```{r using pipes}
#%>% can be replaced with the word "then"
#create new dataframe (data.processed)
#start from original data NTL.phys.data
#filter() - the two lakes we're looking at 
#select() - the columns we want (lakename and sampledate:temperature_c)
#and then we want to create a new column mutate() - for temp in F
NTL.phys.data.processed <- 
  NTL.phys.data %>%
  filter(lakename == "Paul Lake" | lakename == "Peter Lake") %>%
  select(lakename, sampledate:temperature_C) %>%
  mutate(temperature_F = (temperature_C*9/5) + 32)
  
```

Notice that we did not place the dataset name inside the wrangling function but rather at the beginning.

### Saving processed datasets

As soon as you create a processed data set you should save it as a .csv

```{r exporting data}
#use write.csv() to make a new .csv file 
write.csv(NTL.phys.data.PeterPaul1, row.names = FALSE, file = "./Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv")
```

## Closing Discussion

When we wrangle a raw dataset into a processed dataset, we create a code file that contains only the wrangling code. We then save the processed dataset as a new spreadsheet and then create a separate code file to analyze and visualize the dataset. Why do we keep the wrangling code separate from the analysis code?

-> we don't want to do everything in the same document because we'll have a really rmd file and it's a good way to practice having a clean practice to wrangle and process data
--> creates cleaner rscripts - better for reproducibility


